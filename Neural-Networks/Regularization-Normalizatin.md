Batch Normalization & Layer Normalization
-------------------
01. 概述: 两种归一化都是对数据以某个角度或者层面做0均值1方差的处理
02. 动机: 深度学习中的Internal Covariate Shift(内部变量偏移), 使得各层输入不再是独立同分布的;  
(1) 上层参数需要不断适应新的输入数据分布，降低学习速度  
(2) 反向传播时低层神经网络的梯度消失, 收敛变慢乃至无法学习
03. BN把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布(稳定分布)  
(1) BN层让损失函数更平滑  
(2) BN更有利于梯度下降，使得梯度不会出现过大或者过小的梯度值
04. BatchNorm就是通过对batch size这个维度归一化来让分布稳定下来 LayerNorm则是通过对Hidden size这个维度归一